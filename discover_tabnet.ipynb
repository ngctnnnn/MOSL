{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AdIJ0RcAo2AW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdIJ0RcAo2AW",
        "outputId": "63e51123-3870-45f2-bcc5-e33778d927a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-tabnet -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84cae17d-8f94-4097-829c-7267af4613d4",
      "metadata": {
        "id": "84cae17d-8f94-4097-829c-7267af4613d4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from joblib import load\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import getopt\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from pytorch_tabnet.augmentations import ClassificationSMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c15663b-e802-4e04-9945-fb1f2aab73e0",
      "metadata": {
        "id": "5c15663b-e802-4e04-9945-fb1f2aab73e0"
      },
      "source": [
        "# Discover train/test setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U2e8V3E_6YcA",
      "metadata": {
        "id": "U2e8V3E_6YcA"
      },
      "outputs": [],
      "source": [
        "def get_train_test_df(feature_df):\n",
        "    train_size = 0.7\n",
        "    val_size = 0.1\n",
        "    test_size = 0.2\n",
        "\n",
        "    # Split the data into train and temp (temp will later be split into validation and test)\n",
        "    df_train, df_temp = train_test_split(feature_df, test_size=(val_size + test_size), stratify=feature_df['label'], random_state=42)\n",
        "\n",
        "    # Calculate the relative size of validation and test splits from the temp set\n",
        "    relative_test_size = test_size / (val_size + test_size)\n",
        "\n",
        "    # Split the temp set into validation and test sets\n",
        "    df_val, df_test = train_test_split(df_temp, test_size=relative_test_size, stratify=df_temp['label'], random_state=42)\n",
        "\n",
        "    # Check the distribution in each split\n",
        "    print(\"Train label distribution:\\n\", df_train['label'].value_counts(normalize=False))\n",
        "    print(\"Validation label distribution:\\n\", df_val['label'].value_counts(normalize=False))\n",
        "    print(\"Test label distribution:\\n\", df_test['label'].value_counts(normalize=False))\n",
        "\n",
        "    return df_train, df_test, df_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yac0GEe8-umE",
      "metadata": {
        "id": "yac0GEe8-umE"
      },
      "outputs": [],
      "source": [
        "def move_records(df1: pd.DataFrame, df2: pd.DataFrame, head: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    # Select the first 'head' records with label 0\n",
        "    records_to_move = df1[df1['label'] == 0].head(head)\n",
        "\n",
        "    # Append the selected records to df2\n",
        "    df2 = pd.concat([df2, records_to_move], ignore_index=True)\n",
        "\n",
        "    # Drop the selected records from df1 and reset index\n",
        "    df1 = df1.drop(records_to_move.index).reset_index(drop=True)\n",
        "\n",
        "    return df1, df2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffa780c-c18e-481d-b98e-150f8d8d82da",
      "metadata": {
        "id": "6ffa780c-c18e-481d-b98e-150f8d8d82da"
      },
      "source": [
        "# Examine config file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c150acb-8dc0-4d5a-9243-ef9d5ca58e2e",
      "metadata": {
        "id": "1c150acb-8dc0-4d5a-9243-ef9d5ca58e2e"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    model_save_path = \"/content/tabnet_ckpt\" # model ckpt\n",
        "    max_epochs = 100\n",
        "    patience = 20\n",
        "    batch_size = 64\n",
        "    virtual_batch_size = 32\n",
        "    num_workers = 0\n",
        "    weights = 1\n",
        "    drop_last = False\n",
        "    compute_importance = True\n",
        "    p_aug = 0.2\n",
        "    eval_metric = \"accuracy\"\n",
        "    do_save = True\n",
        "    parameters = {\n",
        "        \"gamma\": 1,\n",
        "        \"optimizer_fn\": torch.optim.Adam,\n",
        "        \"optimizer_params\": dict(lr=2e-2),\n",
        "        \"scheduler_params\": {\n",
        "            \"step_size\": 50, # how to use learning rate scheduler\n",
        "            \"gamma\":0.9\n",
        "        },\n",
        "        \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n",
        "        \"mask_type\":'sparsemax', # \"sparsemax\"\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716ddc67-12b3-4302-a7c9-66a88a546887",
      "metadata": {
        "id": "716ddc67-12b3-4302-a7c9-66a88a546887"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55928857-fbbc-48b1-a8d2-4d749b2a1e4e",
      "metadata": {
        "id": "55928857-fbbc-48b1-a8d2-4d749b2a1e4e"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def get_dataset(self, df, do_split=False):\n",
        "        sub_df = df[[\"RNAi_n1\", \"RNAi_n2\", \"CRISPR_n1\", \"CRISPR_n2\", \"label\"]]\n",
        "        X_original = sub_df.iloc[:, :-1].values\n",
        "        y_original = sub_df.iloc[:, -1].values\n",
        "        if do_split:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X_original,\n",
        "                                                              y_original,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state=42)\n",
        "            return X_train, X_val, y_train, y_val\n",
        "        else:\n",
        "            return X_original, y_original\n",
        "\n",
        "    def get_network(self, cfg):\n",
        "        clf = TabNetClassifier(**cfg.parameters)\n",
        "        return clf\n",
        "\n",
        "    def get_aug(self, cfg):\n",
        "        aug = ClassificationSMOTE(p=cfg.p_aug)\n",
        "        return aug\n",
        "\n",
        "    def train(self, cfg, df_train, df_val):\n",
        "        clf = self.get_network(cfg)\n",
        "        X_train, y_train = self.get_dataset(df_train, do_split=False)\n",
        "        X_val, y_val = self.get_dataset(df_val, do_split=False)\n",
        "        aug = self.get_aug(cfg)\n",
        "\n",
        "        save_history = []\n",
        "        clf.fit(\n",
        "            X_train=X_train,\n",
        "            y_train=y_train,\n",
        "            eval_set=[\n",
        "                (X_train, y_train),\n",
        "                (X_val, y_val)\n",
        "            ],\n",
        "            eval_name=['train', 'valid'],\n",
        "            eval_metric=[cfg.eval_metric],\n",
        "            max_epochs=cfg.max_epochs,\n",
        "            patience=cfg.patience,\n",
        "            batch_size=cfg.batch_size,\n",
        "            virtual_batch_size=cfg.virtual_batch_size,\n",
        "            num_workers=cfg.num_workers,\n",
        "            weights=cfg.weights,\n",
        "            drop_last=cfg.drop_last,\n",
        "            augmentations=aug,\n",
        "            compute_importance=cfg.compute_importance\n",
        "        )\n",
        "        save_history.append(clf.history[\"valid_{}\".format(cfg.eval_metric)])\n",
        "        if cfg.do_save:\n",
        "            self.save(clf, cfg.model_save_path)\n",
        "\n",
        "    def save(self, clf, out_path):\n",
        "        clf.save_model(out_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xCoS2OSh27nu",
      "metadata": {
        "id": "xCoS2OSh27nu"
      },
      "outputs": [],
      "source": [
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QWd8HUVw3ARY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWd8HUVw3ARY",
        "outputId": "263ddbc8-f3e6-4b98-d7d0-09bf8bc2b264"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.55298 | train_accuracy: 0.75207 | valid_accuracy: 0.75207 |  0:00:05s\n",
            "epoch 1  | loss: 0.49447 | train_accuracy: 0.78321 | valid_accuracy: 0.78321 |  0:00:11s\n",
            "epoch 2  | loss: 0.47826 | train_accuracy: 0.72706 | valid_accuracy: 0.72706 |  0:00:16s\n",
            "epoch 3  | loss: 0.46489 | train_accuracy: 0.73089 | valid_accuracy: 0.73089 |  0:00:23s\n",
            "epoch 4  | loss: 0.43896 | train_accuracy: 0.75692 | valid_accuracy: 0.75692 |  0:00:28s\n",
            "epoch 5  | loss: 0.4437  | train_accuracy: 0.73281 | valid_accuracy: 0.73281 |  0:00:34s\n",
            "epoch 6  | loss: 0.4323  | train_accuracy: 0.75309 | valid_accuracy: 0.75309 |  0:00:39s\n",
            "epoch 7  | loss: 0.43158 | train_accuracy: 0.8382  | valid_accuracy: 0.8382  |  0:00:45s\n",
            "epoch 8  | loss: 0.42999 | train_accuracy: 0.79176 | valid_accuracy: 0.79176 |  0:00:51s\n",
            "epoch 9  | loss: 0.45555 | train_accuracy: 0.77798 | valid_accuracy: 0.77798 |  0:00:57s\n",
            "epoch 10 | loss: 0.42274 | train_accuracy: 0.75731 | valid_accuracy: 0.75731 |  0:01:02s\n",
            "epoch 11 | loss: 0.43727 | train_accuracy: 0.80362 | valid_accuracy: 0.80362 |  0:01:08s\n",
            "epoch 12 | loss: 0.43088 | train_accuracy: 0.80145 | valid_accuracy: 0.80145 |  0:01:13s\n",
            "epoch 13 | loss: 0.42542 | train_accuracy: 0.81766 | valid_accuracy: 0.81766 |  0:01:18s\n",
            "epoch 14 | loss: 0.42646 | train_accuracy: 0.80503 | valid_accuracy: 0.80503 |  0:01:24s\n",
            "epoch 15 | loss: 0.41911 | train_accuracy: 0.82353 | valid_accuracy: 0.82353 |  0:01:29s\n",
            "epoch 16 | loss: 0.41056 | train_accuracy: 0.79316 | valid_accuracy: 0.79316 |  0:01:35s\n",
            "epoch 17 | loss: 0.40865 | train_accuracy: 0.86615 | valid_accuracy: 0.86615 |  0:01:41s\n",
            "epoch 18 | loss: 0.40268 | train_accuracy: 0.78283 | valid_accuracy: 0.78283 |  0:01:46s\n",
            "epoch 19 | loss: 0.40751 | train_accuracy: 0.80873 | valid_accuracy: 0.80873 |  0:01:52s\n",
            "epoch 20 | loss: 0.39815 | train_accuracy: 0.80439 | valid_accuracy: 0.80439 |  0:01:57s\n",
            "epoch 21 | loss: 0.39983 | train_accuracy: 0.82583 | valid_accuracy: 0.82583 |  0:02:03s\n",
            "epoch 22 | loss: 0.40703 | train_accuracy: 0.77874 | valid_accuracy: 0.77874 |  0:02:08s\n",
            "epoch 23 | loss: 0.3885  | train_accuracy: 0.82072 | valid_accuracy: 0.82072 |  0:02:14s\n",
            "epoch 24 | loss: 0.39944 | train_accuracy: 0.83016 | valid_accuracy: 0.83016 |  0:02:19s\n",
            "epoch 25 | loss: 0.39092 | train_accuracy: 0.74416 | valid_accuracy: 0.74416 |  0:02:25s\n",
            "epoch 26 | loss: 0.39932 | train_accuracy: 0.80911 | valid_accuracy: 0.80911 |  0:02:30s\n",
            "epoch 27 | loss: 0.38857 | train_accuracy: 0.82761 | valid_accuracy: 0.82761 |  0:02:36s\n",
            "epoch 28 | loss: 0.38963 | train_accuracy: 0.80962 | valid_accuracy: 0.80962 |  0:02:41s\n",
            "epoch 29 | loss: 0.39056 | train_accuracy: 0.84726 | valid_accuracy: 0.84726 |  0:02:46s\n",
            "epoch 30 | loss: 0.39986 | train_accuracy: 0.79456 | valid_accuracy: 0.79456 |  0:02:52s\n",
            "epoch 31 | loss: 0.36999 | train_accuracy: 0.82685 | valid_accuracy: 0.82685 |  0:02:57s\n",
            "epoch 32 | loss: 0.39716 | train_accuracy: 0.81192 | valid_accuracy: 0.81192 |  0:03:03s\n",
            "epoch 33 | loss: 0.38773 | train_accuracy: 0.81179 | valid_accuracy: 0.81179 |  0:03:08s\n",
            "epoch 34 | loss: 0.39389 | train_accuracy: 0.8109  | valid_accuracy: 0.8109  |  0:03:14s\n",
            "epoch 35 | loss: 0.39733 | train_accuracy: 0.81319 | valid_accuracy: 0.81319 |  0:03:19s\n",
            "epoch 36 | loss: 0.38817 | train_accuracy: 0.8539  | valid_accuracy: 0.8539  |  0:03:25s\n",
            "epoch 37 | loss: 0.3871  | train_accuracy: 0.80681 | valid_accuracy: 0.80681 |  0:03:31s\n",
            "epoch 38 | loss: 0.38235 | train_accuracy: 0.83042 | valid_accuracy: 0.83042 |  0:03:36s\n",
            "epoch 39 | loss: 0.3989  | train_accuracy: 0.78321 | valid_accuracy: 0.78321 |  0:03:42s\n",
            "epoch 40 | loss: 0.38404 | train_accuracy: 0.77568 | valid_accuracy: 0.77568 |  0:03:47s\n",
            "epoch 41 | loss: 0.39839 | train_accuracy: 0.82123 | valid_accuracy: 0.82123 |  0:03:54s\n",
            "epoch 42 | loss: 0.38523 | train_accuracy: 0.79992 | valid_accuracy: 0.79992 |  0:03:59s\n",
            "epoch 43 | loss: 0.40369 | train_accuracy: 0.83233 | valid_accuracy: 0.83233 |  0:04:04s\n",
            "epoch 44 | loss: 0.38328 | train_accuracy: 0.78653 | valid_accuracy: 0.78653 |  0:04:10s\n",
            "epoch 45 | loss: 0.3992  | train_accuracy: 0.87074 | valid_accuracy: 0.87074 |  0:04:15s\n",
            "epoch 46 | loss: 0.39273 | train_accuracy: 0.82136 | valid_accuracy: 0.82136 |  0:04:21s\n",
            "epoch 47 | loss: 0.37417 | train_accuracy: 0.74174 | valid_accuracy: 0.74174 |  0:04:26s\n",
            "epoch 48 | loss: 0.38298 | train_accuracy: 0.83501 | valid_accuracy: 0.83501 |  0:04:32s\n",
            "epoch 49 | loss: 0.38191 | train_accuracy: 0.78538 | valid_accuracy: 0.78538 |  0:04:37s\n",
            "epoch 50 | loss: 0.38728 | train_accuracy: 0.74467 | valid_accuracy: 0.74467 |  0:04:43s\n",
            "epoch 51 | loss: 0.38239 | train_accuracy: 0.83961 | valid_accuracy: 0.83961 |  0:04:48s\n",
            "epoch 52 | loss: 0.3843  | train_accuracy: 0.80094 | valid_accuracy: 0.80094 |  0:04:53s\n",
            "epoch 53 | loss: 0.37404 | train_accuracy: 0.7864  | valid_accuracy: 0.7864  |  0:04:59s\n",
            "epoch 54 | loss: 0.37252 | train_accuracy: 0.78499 | valid_accuracy: 0.78499 |  0:05:04s\n",
            "epoch 55 | loss: 0.37311 | train_accuracy: 0.79061 | valid_accuracy: 0.79061 |  0:05:10s\n",
            "epoch 56 | loss: 0.38381 | train_accuracy: 0.79125 | valid_accuracy: 0.79125 |  0:05:16s\n",
            "epoch 57 | loss: 0.37673 | train_accuracy: 0.81766 | valid_accuracy: 0.81766 |  0:05:21s\n",
            "epoch 58 | loss: 0.37932 | train_accuracy: 0.78206 | valid_accuracy: 0.78206 |  0:05:27s\n",
            "epoch 59 | loss: 0.37948 | train_accuracy: 0.84637 | valid_accuracy: 0.84637 |  0:05:32s\n",
            "Stop training because you reached max_epochs = 60 with best_epoch = 45 and best_valid_accuracy = 0.87074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved model at /content/tabnet_ckpt.zip\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer()\n",
        "trainer.train(cfg=cfg, df_train=df_train, df_val=df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5e31be-a608-436d-bd19-20858148010a",
      "metadata": {
        "id": "4b5e31be-a608-436d-bd19-20858148010a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "from metrics import compute_acc, compute_f1, compute_specificity, compute_sensitivity\n",
        "\n",
        "\n",
        "class Infer:\n",
        "    def get_pretrained(self, path):\n",
        "        loaded_clf = TabNetClassifier()\n",
        "        loaded_clf.load_model(path)\n",
        "        return loaded_clf\n",
        "\n",
        "    def predict(self, df, pretrained_path, csv_path, reverse = False):\n",
        "        if reverse:\n",
        "            test_df = df[[\"RNAi_n2\", \"RNAi_n1\", \"CRISPR_n2\", \"CRISPR_n1\", \"label\"]]\n",
        "        else:\n",
        "            test_df = df[[\"RNAi_n1\", \"RNAi_n2\", \"CRISPR_n1\", \"CRISPR_n2\", \"label\"]]\n",
        "\n",
        "        X_test = test_df.iloc[:, :-1].values\n",
        "        y_test = test_df.iloc[:, -1].values\n",
        "\n",
        "        clf = self.get_pretrained(pretrained_path)\n",
        "        y_test_pred = clf.predict_proba(X_test)\n",
        "\n",
        "        y_test_preds_softmax = softmax(y_test_pred, axis=1)\n",
        "\n",
        "        y_test_pred_id = np.argmax(y_test_preds_softmax, axis=1)\n",
        "\n",
        "        accuracy = compute_acc(y_test, y_test_pred_id)\n",
        "        spec = compute_specificity(y_test, y_test_pred_id)\n",
        "        sen = compute_sensitivity(y_test, y_test_pred_id)\n",
        "        f1 = compute_f1(y_test, y_test_pred_id)\n",
        "\n",
        "        # print(y_test_pred)\n",
        "        # print(\"###############\")\n",
        "        # print(y_test_preds_softmax)\n",
        "\n",
        "        probas = []\n",
        "        for idx in range(len(y_test_pred_id)):\n",
        "            probas.append(y_test_preds_softmax[idx][y_test_pred_id[idx]])\n",
        "\n",
        "        df[\"prediction\"] = y_test_pred_id\n",
        "        df[\"probability\"] = probas\n",
        "        df = df.sort_values(\"X\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        print(\"TabNet evaluation\")\n",
        "        print(classification_report(y_test, y_test_pred_id, target_names=['Negative', 'Positive']))\n",
        "        print(\"Accuracy:\", accuracy)\n",
        "        print(\"Specificity:\", spec)\n",
        "        print(\"Sensitivity:\", sen)\n",
        "        print(\"F1:\", f1)\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_test_pred_id, labels=[0, 1])\n",
        "        df_cm = pd.DataFrame(cm, columns=[\"Negative\", \"Positive\"], index=[\"Negative\", \"Positive\"])\n",
        "        df_cm['Negative'] = df_cm['Negative'].astype(np.int64)\n",
        "        df_cm['Positive'] = df_cm['Positive'].astype(np.int64)\n",
        "        df_cm.index.name = 'Actual'\n",
        "        df_cm.columns.name = 'Predicted'\n",
        "        plt.figure(figsize = (12, 10))\n",
        "        sn.set(font_scale=1)\n",
        "        sn.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='d')# font size\n",
        "\n",
        "        return y_test_pred_id\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
